{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cybersecurity-excellence-awards.com/wp-content/uploads/2017/06/366812.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1><center>Darwin Unsupervised Model Building </center></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to getting started, there are a few things you want to do:\n",
    "1. Enter your username and password to ensure that you're able to log in successfully\n",
    "2. Set the path to your dataset. If left unfilled, you will be testing an example dataset on the server. \n",
    "3. Set the dataset path for feature importance\n",
    "  - For global feature importance, the dataset path remains the same as your original dataset\n",
    "  - For individual row's feature importance, you need to specify a path to a dataset that contains no more than 500       rows.\n",
    "\n",
    "Once you're up and running, here are a few things to be mindful of:\n",
    "1. For every run, check the job status (i.e. requested, failed, running, completed) and wait for job to complete before proceeding. \n",
    "2. If you're not satisfied with your model and think that Darwin can benefit from extra training, use the resume function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import Image\n",
    "from time import sleep\n",
    "import os\n",
    "\n",
    "from amb_sdk.sdk import DarwinSdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "In the cell below, you need to set up the following variables:\n",
    " - path: the designated path to your dataset, default to be an example dataset \"pulsar.csv\" which you should be able to find in the Darwin SDK /sets folder\n",
    " - user, password: your credentials to log in the next time you use Darwin SDK. These should be inside of your trial email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set local data path to files\n",
    "path = '../../sets/'\n",
    "\n",
    "# View data snippet\n",
    "df = pd.read_csv(os.path.join(path, 'pulsars.csv'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log in with the username and password that were attached to your trial email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DarwinSdk()\n",
    "ds.set_url('https://amb-trial-api.sparkcognition.com/v1/')\n",
    "status, msg = ds.auth_login_user('username','password')\n",
    "if not status:\n",
    "    print(msg)\n",
    "else:\n",
    "    print(\"You are now logged in!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Data\n",
    "After setting up the dataset path, the next step is to upload the dataset from your local device to the server. In the cell below, you need to specify the dataset_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload dataset\n",
    "ds.delete_all_models()\n",
    "ds.delete_all_datasets()\n",
    "dataset_name = 'pulsars'\n",
    "status, dataset = ds.upload_dataset(os.path.join(path, 'pulsars.csv'), dataset_name)\n",
    "if not status:\n",
    "    print(dataset)\n",
    "else:\n",
    "    print(\"Data uploaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Train Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build unsupervised models, which cluster data and perform anomaly detection, Darwin goes through the following steps:\n",
    "1. Determines an approximate number of clusters to start with using a single pass with a hierarchical method\n",
    "2. Iterates on subsets of the data using a Spectral-Net algorithm to determine the ideal number of clusters\n",
    "3. Proceeds to cluster the data using a Spectral-Net approach\n",
    "\n",
    "In the cell below, specify the parameters used to create the model:\n",
    "- model: the name of your model\n",
    "- max_epochs: the number of epochs to train the model, one epoch indicates one scan of the entire dataset\n",
    "- n_clusters: the number of clusters, either an integer or 'auto', if left with 'auto', the unsupervised algorithm will compute a number for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = \"your_model_name\"\n",
    "max_epochs = 20\n",
    "n_clusters = 2\n",
    "status, job_id = ds.create_model(dataset_names=dataset_name,\n",
    "                                 model_name=model,\n",
    "                                 max_epochs=max_epochs,\n",
    "                                 n_clusters=n_clusters)\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up job status\n",
    "ds.lookup_job_status_name(job_id['job_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up the model\n",
    "ds.lookup_model_name(job_id['model_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Training (Optional)\n",
    "Run the following cell for extra training, no need to specify parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train some more\n",
    "extra_epochs = 10\n",
    "status, job_id = ds.resume_training_model(dataset_names=dataset_name,\n",
    "                                          model_name=model,\n",
    "                                          max_epochs=extra_epochs,\n",
    "                                          n_clusters=n_clusters)\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "Run the following cell for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "status, artifact = ds.run_model(dataset_name, \n",
    "                                model, \n",
    "                                supervised=False)\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "status, pred_file = ds.download_artifact(artifact['artifact_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View prediction\n",
    "pred_file.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, analyze_id = ds.analyze_data(dataset_name, \n",
    "                                     job_name = 'Darwin_analyze_data_job', \n",
    "                                     artifact_name = 'Darwin_analyze_data_artifact')\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job('Darwin_analyze_data_job')\n",
    "else:\n",
    "    print(analyze_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.lookup_job_status_name('Darwin_analyze_data_job')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, analyze_results = ds.download_artifact('Darwin_analyze_data_artifact')\n",
    "analyze_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Model\n",
    "Analyze model provides feature importance ranked by the model. It indicates a general view of which features pose a bigger impact on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, analyze_id = ds.analyze_model(job_id['model_name'], \n",
    "                                      job_name='Darwin_analyze_model_job', \n",
    "                                      artifact_name='Darwin_analyze_model_artifact')\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job('Darwin_analyze_model_job')\n",
    "else:\n",
    "    print(analyze_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.lookup_job_status_name('Darwin_analyze_model_job')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloade and print the top 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, feature_importance = ds.download_artifact('Darwin_analyze_model_artifact')\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Prediction\n",
    "Different from Analyze Model, the Analyze Prediction provides a way to analyze feature importance for each data point. The output estimates how each feature added or subtracted from a known base-value to result in the overall prediction that was made. <br>\n",
    "**You need to set the path to a dataset which contains all the samples you want to analyze (max rows = 500)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the data that you are interested in feature importance (max: 500 rows)\n",
    "dataset_name = 'pulsars_predict'\n",
    "path = '../../sets/'\n",
    "status, response = ds.upload_dataset(os.path.join(path, 'pulsars_predict.csv'), dataset_name)\n",
    "print(status)\n",
    "print(response)\n",
    "if status:\n",
    "    dataset_by_row=response['dataset_name']\n",
    "else:\n",
    "    print(\"Upload data failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, analyze_id = ds.analyze_predictions(job_id['model_name'], \n",
    "                                            'pulsars_predict',\n",
    "                                            job_name='Analyze_prediction_job', \n",
    "                                            artifact_name='Analyze_prediction_artifact')\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job('Analyze_prediction_job')\n",
    "else:\n",
    "    print(analyze_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.lookup_job_status_name('Analyze_prediction_job')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and print the top 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, feature_importance = ds.download_artifact('Analyze_prediction_artifact')\n",
    "feature_importance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
