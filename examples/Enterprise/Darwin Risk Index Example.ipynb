{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cybersecurity-excellence-awards.com/wp-content/uploads/2017/06/366812.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Darwin Risk Building</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to getting started:\n",
    "\n",
    "First, if you have just received a new api key from support, you will need to register your key and create a new user (see Register user cell)\n",
    "\n",
    "Second, in the Environment Variables cell: \n",
    "1. Set your username and password to ensure that you're able to log in successfully\n",
    "2. Set the path to the location of your datasets if you are using your own data.  The path is set for the examples.\n",
    "3. Set the dataset names accordingly\n",
    "\n",
    "Here are a few things to be mindful of:\n",
    "1. For every run, check the job status (i.e. requested, failed, running, completed) and wait for job to complete before proceeding. \n",
    "2. If you're not satisfied with your model and think that Darwin can benefit from extra training, use the resume function.\n",
    "\n",
    "Risk: Given the failure datetime, riskindex will include from 0 (healthy) to 1 (failure) with given leadtime and functional form. Note that until the riskindex becomes 1 the system has not failed yet. Riskindex can be thought as a inverse of the remaining time to the future failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Darwin SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amb_sdk.sdk import DarwinSdk\n",
    "ds = DarwinSdk()\n",
    "ds.set_url('https://darwin-api.sparkcognition.com/v1/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register user (if needed, read above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only if you have a new api-key and \n",
    "# no registered users - fill in the appropriate fields then execute\n",
    "\n",
    "#Enter your support provided api key and api key password below to register/create new users\n",
    "api_key = ''\n",
    "api_key_pw = ''\n",
    "status, msg = ds.auth_login(api_key_pw, api_key)\n",
    "if not status:\n",
    "    print(msg)\n",
    "\n",
    "#Create a new user \n",
    "status, msg = ds.auth_register_user('username', 'password','email@emailaddress.com')\n",
    "if not status:\n",
    "    print(msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set your user id and password accordingly\n",
    "USER=\"[your Darwin user id]\"\n",
    "PW=\"[your Darwin password]\"\n",
    "\n",
    "# Set path to datasets - The default below assumes Jupyter was started from amb-sdk/examples/Enterprise/\n",
    "PATH_TO_DATASET='../../sets/'\n",
    "FAILURE_DATASET='failures.csv'\n",
    "TIME_SERIES_DATASET='sensor_ts.csv'\n",
    "\n",
    "# Create a file name for your asset risk file that will be created in this workflow.  A timestamp \n",
    "# is used to create a unique name in the event you execute the workflow multiple times or with \n",
    "# different datasets.  File names must be unique in Darwin.\n",
    "import datetime\n",
    "ts = '{:%Y%m%d%H%M%S}'.format(datetime.datetime.now())\n",
    "ASSETRISK='assetrisk-' + ts + '.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Login "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, msg = ds.auth_login_user(USER,PW)\n",
    "if not status:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload data (failure data and timeseries data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploads and assigns filenames to uploaded datasets\n",
    "# failure date data\n",
    "(code, response) = ds.upload_dataset(PATH_TO_DATASET+FAILURE_DATASET, 'unittest-failures-data')\n",
    "if code == False:\n",
    "    print(response)\n",
    "    \n",
    "# timeseries data\n",
    "(code, response) = ds.upload_dataset(PATH_TO_DATASET+TIME_SERIES_DATASET, 'unittest-timeseries-data')\n",
    "if code == False:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create riskindex\n",
    "1. The lead time is in the unit of second (the example is for half week).\n",
    "2. The lead time is the half width of the riskindex time period from 0 to 1\n",
    "2. shutdown_column is the datetime when the system gets failed.\n",
    "3. return_column is the datetime when the system return back healthy.\n",
    "4. shutdiwn_column and return_column are in the failure dataset\n",
    "5. functional_form is the risk function shape including sigmoid, step, linear, exponetial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_time = 3600 * 24 * 7 // 2 # half week risk \n",
    "(code, response) = ds.create_risk_info('unittest-failures-data', 'unittest-timeseries-data',\n",
    "                                        return_column=\"Date Returned to Service\",\n",
    "                                        shutdown_column=\"Shutdown Date\",\n",
    "                                        lead_time=lead_time, functional_form=\"sigmoid\")\n",
    "if code == False:\n",
    "    print(response)\n",
    "    \n",
    "ds.wait_for_job(response['job_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the artifact (riskindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(status, response) = ds.download_artifact(response['artifact_name'])\n",
    "\n",
    "if code == False:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_risk = pd.read_csv(response['filename'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read local datasets again for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read timeseries data for datetime index\n",
    "df_ts = pd.read_csv(PATH_TO_DATASET+TIME_SERIES_DATASET)\n",
    "df_ts['datetime'] = pd.to_datetime(df_ts['datetime'], errors='coerce')\n",
    "\n",
    "# concatenate two dfs\n",
    "df = pd.concat([df_ts, df_risk], axis=1)\n",
    "\n",
    "# set datetime index\n",
    "df= df.set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['risk'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the failures\n",
    "df_failure = pd.read_csv(PATH_TO_DATASET+FAILURE_DATASET)\n",
    "df_failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's zoom-in the date around 2012-02-10 failure date\n",
    "df['risk']['2012-01-25':'2012-02-12'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a full csv file and Upload for suprevised training\n",
    "Now we generated the dataset which includes the risk index as a supervised label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View full file upon which you can build a model and convert to csv\n",
    "df.to_csv(ASSETRISK)\n",
    "df.head()\n",
    "\n",
    "#Upload Full CSV to Darwin for predictions\n",
    "(code, response) = ds.upload_dataset(ASSETRISK)\n",
    "if code == False:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model\n",
    "Now the dataset becomes the same as supervised model.\n",
    "One can consult the supervised notebook for more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"risk\"\n",
    "model = target + \"_model0\" + '-' + ts\n",
    "(status, job_id) = ds.create_model(dataset_names = ASSETRISK, \\\n",
    "                                 target = target, \\\n",
    "                                 model_name =  model, \\\n",
    "                                 max_train_time = '00:10'#,\\\n",
    "                                )\n",
    "if status == False:\n",
    "    print(response)\n",
    "\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.lookup_job_status_name(job_id['job_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Predictions    \n",
    "status, artifact = ds.run_model(ASSETRISK, model)\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get predictions\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename\n",
    "prediction = prediction.rename(columns={target:target+'_pred' })\n",
    "\n",
    "df = df.reset_index()\n",
    "\n",
    "# concatenate two df\n",
    "df = pd.concat([df, prediction], axis=1)\n",
    "\n",
    "# set datetime index\n",
    "df= df.set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot predictions vs actual\n",
    "df[target].plot()\n",
    "df[target+'_pred'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all models and datasets\n",
    "ds.delete_all_datasets()\n",
    "ds.delete_all_models()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
