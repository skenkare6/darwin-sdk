{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cybersecurity-excellence-awards.com/wp-content/uploads/2017/06/366812.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Darwin Normal Behavior Modeling (NBM) Example </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior to getting started:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, \n",
    "<br>if you have just received a new api key from support, you will need to register your key and create a new user (see Register user cell)\n",
    "\n",
    "Second, in the Environment Variables cell: \n",
    "1. Set your username and password to ensure that you're able to log in successfully\n",
    "2. Set the path to the location of your datasets if you are using your own data.  The path is set for the examples.\n",
    "\n",
    "Here are a few things to be mindful of:\n",
    "1. For every run, check the job status (i.e. requested, failed, running, completed) and wait for job to complete before proceeding. \n",
    "2. If you're not satisfied with your model and think that Darwin can do better by exploring a larger search space, use the resume function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Darwin SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amb_sdk.sdk import DarwinSdk\n",
    "ds = DarwinSdk()\n",
    "ds.set_url('https://darwin-api.sparkcognition.com/v1/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register user (if needed, read above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only if you have a new api-key and \n",
    "# no registered users - fill in the appropriate fields then execute\n",
    "\n",
    "#Enter your support provided api key and api key password below to register/create new users\n",
    "api_key = ''\n",
    "api_key_pw = ''\n",
    "status, msg = ds.auth_login(api_key_pw, api_key)\n",
    "if not status:\n",
    "    print(msg)\n",
    "\n",
    "#Create a new user\n",
    "status, msg = ds.auth_register_user('username', 'password','email@emailaddress.com')\n",
    "if not status:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set your user id and password accordingly\n",
    "USER=\"[your Darwin user id]\"\n",
    "PW=\"[your Darwin password]\"\n",
    "\n",
    "# Set path to datasets - The default below assumes Jupyter was started from amb-sdk/examples/Enterprise/\n",
    "# Modify accordingly if you wish to use your own data\n",
    "PATH_TO_DATASET = '../../sets/'\n",
    "TRAIN_DATASET = 'wind_turbine.csv'\n",
    "\n",
    "# A timestamp is used to create a unique name in the event you execute the workflow multiple times or with \n",
    "# different datasets.  File names must be unique in Darwin.\n",
    "import datetime\n",
    "ts = '{:%Y%m%d%H%M%S}'.format(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import Image\n",
    "from time import sleep\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, msg = ds.auth_login_user(USER,PW)\n",
    "if not status:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "**Read dataset and view a file snippet**\n",
    "<br>After setting up the dataset path, the next step is to upload the dataset from your local device to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview dataset\n",
    "df = pd.read_csv(os.path.join(PATH_TO_DATASET, TRAIN_DATASET))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upload dataset to Darwin**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Upload dataset\n",
    "status, dataset = ds.upload_dataset(os.path.join(PATH_TO_DATASET, TRAIN_DATASET))\n",
    "print(status)\n",
    "print(dataset)\n",
    "\n",
    "if not status:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean dataset\n",
    "status, job_id = ds.clean_data(TRAIN_DATASET)\n",
    "\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Train Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build a model that will learn the normal behavior of an asset based on a failure date.<br> The failure date in our example dataset is 8/24/15. <br> You will have to specify a different failure date for your custom dataset. <br> You can also specify a recovery_dates when the asset comes back online.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = \"model\" + \"-\" + ts\n",
    "status, job_id = ds.create_model(dataset_names = TRAIN_DATASET, \\\n",
    "                                 failure_dates = ['08/24/15'], \\\n",
    "                                 model_name =  model, \\\n",
    "                                 nbm = True, \\\n",
    "                                 max_train_time = '00:10')\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'], time_limit=720)\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Training (Optional)\n",
    "Run the following cell for extra training, no need to specify parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train some more\n",
    "status, job_id = ds.resume_training_model(dataset_names = TRAIN_DATASET,\n",
    "                                          model_name = model,\n",
    "                                          max_train_time = '00:10')\n",
    "                                          \n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'],time_limit=720)\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Model\n",
    "Analyze model provides feature importance ranked by the model. <br> It indicates a general view of which features pose a bigger impact on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Retrieve feature importance of built model\n",
    "#status, artifact = ds.analyze_model(model)\n",
    "status, analyze_id = ds.analyze_model(job_id['model_name'], \n",
    "                                      job_name='Darwin_analyze_model_job-' + ts, \n",
    "                                      artifact_name='Darwin_analyze_model_artifact-' + ts)\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job(analyze_ids['job_name'])\n",
    "else:\n",
    "    print(analyze_id)\n",
    "status, feature_importance = ds.download_artifact(analyze_id['artifact_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, feature_importance = ds.download_artifact(analyze_id['artifact_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "**Perform model prediction on the the training dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(TRAIN_DATASET, model)\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download predictions from Darwin's server.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "status, prediction = ds.download_artifact(artifact['artifact_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create plots showing the risk index prediction of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot the risk predictions\n",
    "prediction.set_index(pd.to_datetime(df['timestamp']), inplace=True)\n",
    "prediction.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find out which machine learning model did Darwin use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, model_type = ds.lookup_model_name(model)\n",
    "print(model_type['description']['best_genome'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
